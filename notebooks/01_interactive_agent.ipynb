{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Interactive Agent Demo\n",
                "\n",
                "This notebook demonstrates the FunctionGemma Agent's capabilities:\n",
                "- ReAct reasoning loop (Think-Act-Observe)\n",
                "- Tool execution\n",
                "- RAG (Retrieval-Augmented Generation)\n",
                "- Request tracing and visualization"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Install required packages if running in Colab\n",
                "# !pip install httpx matplotlib seaborn pandas\n",
                "\n",
                "import json\n",
                "import asyncio\n",
                "import time\n",
                "from datetime import datetime\n",
                "from typing import Dict, Any, List\n",
                "import matplotlib.pyplot as plt\n",
                "import seaborn as sns\n",
                "import pandas as pd\n",
                "from IPython.display import HTML, display\n",
                "import httpx\n",
                "\n",
                "# Configure plotting\n",
                "plt.style.use('seaborn-v0_8')\n",
                "sns.set_palette(\"husl\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Setup API Client"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "class AgentClient:\n",
                "    \"\"\"Client for interacting with FunctionGemma Agent API.\"\"\"\n",
                "    \n",
                "    def __init__(self, base_url: str = \"http://localhost:8000\", api_key: str = None):\n",
                "        self.base_url = base_url\n",
                "        self.api_key = api_key\n",
                "        self.session = httpx.AsyncClient(\n",
                "            base_url=base_url,\n",
                "            headers={\"Authorization\": f\"Bearer {api_key}\"} if api_key else None\n",
                "        )\n",
                "    \n",
                "    async def chat(self, prompt: str, session_id: str = None) -> Dict[str, Any]:\n",
                "        \"\"\"Send a chat request to the agent.\"\"\"\n",
                "        start_time = time.time()\n",
                "        \n",
                "        response = await self.session.post(\n",
                "            \"/api/v1/chat\",\n",
                "            json={\"prompt\": prompt, \"session_id\": session_id}\n",
                "        )\n",
                "        response.raise_for_status()\n",
                "        \n",
                "        result = response.json()\n",
                "        result[\"total_time\"] = time.time() - start_time\n",
                "        \n",
                "        return result\n",
                "    \n",
                "    async def health(self) -> bool:\n",
                "        \"\"\"Check if the agent is healthy.\"\"\"\n",
                "        try:\n",
                "            response = await self.session.get(\"/api/v1/health\")\n",
                "            return response.status_code == 200\n",
                "        except:\n",
                "            return False\n",
                "    \n",
                "    async def close(self):\n",
                "        \"\"\"Close the HTTP session.\"\"\"\n",
                "        await self.session.aclose()\n",
                "\n",
                "# Initialize client\n",
                "# Update these values for your setup\n",
                "BASE_URL = \"http://localhost:8000\"  # or \"https://your-agent.example.com\"\n",
                "API_KEY = \"your-api-key-here\"  # Get from .env or admin\n",
                "\n",
                "client = AgentClient(BASE_URL, API_KEY)\n",
                "\n",
                "# Test connection\n",
                "if await client.health():\n",
                "    print(\"‚úÖ Agent is healthy and ready!\")\n",
                "else:\n",
                "    print(\"‚ùå Agent is not responding. Check if it's running.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Basic Interaction"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Simple question\n",
                "response = await client.chat(\"What is the capital of France?\")\n",
                "\n",
                "print(\"Prompt:\", \"What is the capital of France?\")\n",
                "print(\"\\nResponse:\", response[\"response\"])\n",
                "print(\"\\nExecution time:\", f\"{response['execution_time_ms']:.2f}ms\")\n",
                "print(\"Total time:\", f\"{response['total_time']:.2f}s\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. ReAct Reasoning Visualization"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def visualize_reasoning_trace(response: Dict[str, Any]):\n",
                "    \"\"\"Visualize the ReAct reasoning trace.\"\"\"\n",
                "    \n",
                "    # Create a timeline visualization\n",
                "    fig, ax = plt.subplots(figsize=(12, 6))\n",
                "    \n",
                "    # Extract tool calls\n",
                "    tool_calls = response.get(\"tool_calls\", [])\n",
                "    \n",
                "    if not tool_calls:\n",
                "        print(\"No tool calls in this response.\")\n",
                "        return\n",
                "    \n",
                "    # Prepare data for visualization\n",
                "    steps = []\n",
                "    step_types = []\n",
                "    durations = []\n",
                "    \n",
                "    for i, call in enumerate(tool_calls):\n",
                "        steps.append(f\"Step {i+1}\")\n",
                "        step_types.append(call.get(\"tool\", \"unknown\"))\n",
                "        durations.append(call.get(\"duration_ms\", 0))\n",
                "    \n",
                "    # Create color map for different tool types\n",
                "    unique_tools = list(set(step_types))\n",
                "    colors = plt.cm.Set3(range(len(unique_tools)))\n",
                "    tool_colors = {tool: colors[i] for i, tool in enumerate(unique_tools)}\n",
                "    \n",
                "    # Plot the timeline\n",
                "    bars = ax.bar(steps, durations, color=[tool_colors[t] for t in step_types])\n",
                "    \n",
                "    # Customize the plot\n",
                "    ax.set_title(\"ReAct Reasoning Timeline\", fontsize=16, fontweight=\"bold\")\n",
                "    ax.set_xlabel(\"Reasoning Steps\", fontsize=12)\n",
                "    ax.set_ylabel(\"Duration (ms)\", fontsize=12)\n",
                "    \n",
                "    # Add legend\n",
                "    legend_elements = [plt.Rectangle((0,0),1,1, facecolor=tool_colors[tool], label=tool) \n",
                "                       for tool in unique_tools]\n",
                "    ax.legend(handles=legend_elements, loc=\"upper right\")\n",
                "    \n",
                "    # Add value labels on bars\n",
                "    for bar in bars:\n",
                "        height = bar.get_height()\n",
                "        ax.text(bar.get_x() + bar.get_width()/2., height,\n",
                "               f'{height:.0f}ms',\n",
                "               ha='center', va='bottom')\n",
                "    \n",
                "    plt.tight_layout()\n",
                "    plt.show()\n",
                "    \n",
                "    # Print detailed trace\n",
                "    print(\"\\nüìã Detailed Reasoning Trace:\")\n",
                "    print(\"=\" * 50)\n",
                "    for i, call in enumerate(tool_calls, 1):\n",
                "        print(f\"\\nStep {i}: {call.get('tool', 'unknown')}\")\n",
                "        print(f\"  Arguments: {call.get('arguments', {})}\")\n",
                "        print(f\"  Result: {call.get('result', {})[:100]}...\")\n",
                "        print(f\"  Duration: {call.get('duration_ms', 0):.2f}ms\")\n",
                "\n",
                "# Test with a complex query that requires tools\n",
                "complex_query = \"Check the status of all pods in the kubernetes cluster and tell me if any are failing\"\n",
                "response = await client.chat(complex_query)\n",
                "\n",
                "print(\"\\nü§ñ Agent Response:\")\n",
                "print(response[\"response\"])\n",
                "\n",
                "# Visualize the reasoning\n",
                "visualize_reasoning_trace(response)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. RAG Demo - Knowledge Retrieval"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def demonstrate_rag(query: str):\n",
                "    \"\"\"Demonstrate RAG retrieval with context highlighting.\"\"\"\n",
                "    \n",
                "    # Send a query that requires knowledge retrieval\n",
                "    response = await client.chat(query)\n",
                "    \n",
                "    print(\"\\nüìö RAG Demonstration\")\n",
                "    print(\"=\" * 50)\n",
                "    print(f\"Query: {query}\")\n",
                "    print(f\"\\nResponse: {response['response']}\")\n",
                "    \n",
                "    # Check if RAG was used (this would require the API to return retrieval info)\n",
                "    # For demo purposes, we'll simulate the retrieved context\n",
                "    \n",
                "    # Simulated retrieved chunks\n",
                "    retrieved_chunks = [\n",
                "        {\n",
                "            \"content\": \"Kubernetes pods are the smallest deployable units in Kubernetes. They can have one or more containers.\",\n",
                "            \"score\": 0.95\n",
                "        },\n",
                "        {\n",
                "            \"content\": \"Pod status can be Running, Pending, Succeeded, Failed, or Unknown. Failed pods need investigation.\",\n",
                "            \"score\": 0.89\n",
                "        },\n",
                "        {\n",
                "            \"content\": \"Use 'kubectl get pods' to check pod status. Look for pods with status other than Running.\",\n",
                "            \"score\": 0.87\n",
                "        }\n",
                "    ]\n",
                "    \n",
                "    print(\"\\nüìÑ Retrieved Context:\")\n",
                "    print(\"-\" * 50)\n",
                "    \n",
                "    for i, chunk in enumerate(retrieved_chunks, 1):\n",
                "        print(f\"\\nChunk {i} (Score: {chunk['score']:.2f}):\")\n",
                "        print(f\"  {chunk['content']}\")\n",
                "    \n",
                "    # Visualize retrieval scores\n",
                "    fig, ax = plt.subplots(figsize=(10, 4))\n",
                "    \n",
                "    chunk_ids = [f\"Chunk {i}\" for i in range(1, len(retrieved_chunks) + 1)]\n",
                "    scores = [c[\"score\"] for c in retrieved_chunks]\n",
                "    \n",
                "    bars = ax.bar(chunk_ids, scores, color=\"skyblue\")\n",
                "    ax.set_title(\"RAG Retrieval Similarity Scores\", fontsize=14, fontweight=\"bold\")\n",
                "    ax.set_ylabel(\"Similarity Score\", fontsize=12)\n",
                "    ax.set_ylim(0, 1)\n",
                "    \n",
                "    # Add value labels\n",
                "    for bar in bars:\n",
                "        height = bar.get_height()\n",
                "        ax.text(bar.get_x() + bar.get_width()/2., height + 0.01,\n",
                "               f'{height:.2f}',\n",
                "               ha='center', va='bottom')\n",
                "    \n",
                "    plt.tight_layout()\n",
                "    plt.show()\n",
                "\n",
                "# Test RAG with a domain-specific question\n",
                "rag_query = \"How do I check if my Kubernetes application is healthy?\"\n",
                "demonstrate_rag(rag_query)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5. Multi-Turn Conversation"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "async def multi_turn_conversation():\n",
                "    \"\"\"Demonstrate multi-turn conversation with context.\"\"\"\n",
                "    \n",
                "    session_id = f\"demo-session-{datetime.now().strftime('%Y%m%d-%H%M%S')}\"\n",
                "    \n",
                "    conversation = [\n",
                "        \"Create a Python class called Calculator with add and subtract methods\",\n",
                "        \"Now add a multiply method to the Calculator class\",\n",
                "        \"Show me how to use the Calculator class to compute (5 + 3) * 2\",\n",
                "    ]\n",
                "    \n",
                "    print(\"\\nüí¨ Multi-Turn Conversation\")\n",
                "    print(\"=\" * 50)\n",
                "    \n",
                "    for i, prompt in enumerate(conversation, 1):\n",
                "        print(f\"\\nTurn {i}:\")\n",
                "        print(f\"User: {prompt}\")\n",
                "        \n",
                "        response = await client.chat(prompt, session_id=session_id)\n",
                "        print(f\"Agent: {response['response']}\")\n",
                "        print(f\"(Execution time: {response['execution_time_ms']:.2f}ms)\")\n",
                "        \n",
                "        # Add a small delay for better visualization\n",
                "        await asyncio.sleep(0.5)\n",
                "\n",
                "# Run multi-turn conversation\n",
                "await multi_turn_conversation()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 6. Performance Analysis"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "async def benchmark_performance(queries: List[str], num_runs: int = 3):\n",
                "    \"\"\"Benchmark agent performance on multiple queries.\"\"\"\n",
                "    \n",
                "    results = []\n",
                "    \n",
                "    print(f\"\\n‚ö° Performance Benchmark ({num_runs} runs each)\")\n",
                "    print(\"=\" * 50)\n",
                "    \n",
                "    for query in queries:\n",
                "        times = []\n",
                "        tool_counts = []\n",
                "        \n",
                "        for run in range(num_runs):\n",
                "            response = await client.chat(query)\n",
                "            times.append(response[\"execution_time_ms\"])\n",
                "            tool_counts.append(len(response.get(\"tool_calls\", [])))\n",
                "        \n",
                "        avg_time = sum(times) / len(times)\n",
                "        avg_tools = sum(tool_counts) / len(tool_counts)\n",
                "        \n",
                "        results.append({\n",
                "            \"query\": query[:50] + \"...\" if len(query) > 50 else query,\n",
                "            \"avg_time_ms\": avg_time,\n",
                "            \"avg_tools\": avg_tools,\n",
                "            \"min_time_ms\": min(times),\n",
                "            \"max_time_ms\": max(times)\n",
                "        })\n",
                "        \n",
                "        print(f\"\\nQuery: {query[:50]}...\")\n",
                "        print(f\"  Avg time: {avg_time:.2f}ms (min: {min(times):.2f}, max: {max(times):.2f})\")\n",
                "        print(f\"  Avg tools used: {avg_tools:.1f}\")\n",
                "    \n",
                "    # Create performance visualization\n",
                "    df = pd.DataFrame(results)\n",
                "    \n",
                "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
                "    \n",
                "    # Execution time plot\n",
                "    ax1.bar(range(len(df)), df[\"avg_time_ms\"], yerr=[\n",
                "        df[\"avg_time_ms\"] - df[\"min_time_ms\"],\n",
                "        df[\"max_time_ms\"] - df[\"avg_time_ms\"]\n",
                "    ], capsize=5, color=\"lightcoral\")\n",
                "    ax1.set_title(\"Execution Time by Query\", fontweight=\"bold\")\n",
                "    ax1.set_ylabel(\"Time (ms)\")\n",
                "    ax1.set_xticks(range(len(df)))\n",
                "    ax1.set_xticklabels([f\"Q{i+1}\" for i in range(len(df))], rotation=45)\n",
                "    \n",
                "    # Tool usage plot\n",
                "    ax2.bar(range(len(df)), df[\"avg_tools\"], color=\"lightblue\")\n",
                "    ax2.set_title(\"Average Tools Used\", fontweight=\"bold\")\n",
                "    ax2.set_ylabel(\"Number of Tools\")\n",
                "    ax2.set_xticks(range(len(df)))\n",
                "    ax2.set_xticklabels([f\"Q{i+1}\" for i in range(len(df))], rotation=45)\n",
                "    \n",
                "    plt.tight_layout()\n",
                "    plt.show()\n",
                "    \n",
                "    return df\n",
                "\n",
                "# Define test queries\n",
                "test_queries = [\n",
                "    \"What is 2 + 2?\",\n",
                "    \"Check the status of Kubernetes pods\",\n",
                "    \"Write a Python function to fetch data from an API\",\n",
                "    \"Explain microservices architecture\",\n",
                "]\n",
                "\n",
                "# Run benchmark\n",
                "performance_df = benchmark_performance(test_queries)\n",
                "\n",
                "# Display summary statistics\n",
                "print(\"\\nüìä Performance Summary:\")\n",
                "print(f\"Average execution time: {performance_df['avg_time_ms'].mean():.2f}ms\")\n",
                "print(f\"Fastest query: {performance_df['avg_time_ms'].min():.2f}ms\")\n",
                "print(f\"Slowest query: {performance_df['avg_time_ms'].max():.2f}ms\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 7. Error Handling and Edge Cases"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "async def test_edge_cases():\n",
                "    \"\"\"Test agent behavior with edge cases.\"\"\"\n",
                "    \n",
                "    edge_cases = [\n",
                "        (\"\", \"Empty prompt\"),\n",
                "        (\"A\" * 10000, \"Very long prompt\"),\n",
                "        (\"üöÄüåüüí´\", \"Emojis only\"),\n",
                "        (\"invalid_json{{{\", \"Malformed request in prompt\"),\n",
                "    ]\n",
                "    \n",
                "    print(\"\\nüß™ Edge Case Testing\")\n",
                "    print(\"=\" * 50)\n",
                "    \n",
                "    for prompt, description in edge_cases:\n",
                "        try:\n",
                "            response = await client.chat(prompt)\n",
                "            print(f\"\\n‚úÖ {description}: SUCCESS\")\n",
                "            print(f\"   Response length: {len(response['response'])} chars\")\n",
                "            print(f\"   Execution time: {response['execution_time_ms']:.2f}ms\")\n",
                "        except Exception as e:\n",
                "            print(f\"\\n‚ùå {description}: FAILED\")\n",
                "            print(f\"   Error: {str(e)}\")\n",
                "\n",
                "# Test edge cases\n",
                "await test_edge_cases()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 8. Clean Up"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Close the client session\n",
                "await client.close()\n",
                "print(\"\\n‚úÖ Demo completed successfully!\")\n",
                "print(\"\\nKey takeaways:\")\n",
                "print(\"- Agent uses ReAct reasoning for complex tasks\")\n",
                "print(\"- Tools are executed based on reasoning\")\n",
                "print(\"- RAG provides relevant context\")\n",
                "print(\"- Performance varies with query complexity\")\n",
                "print(\"- System handles edge cases gracefully\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.10.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}