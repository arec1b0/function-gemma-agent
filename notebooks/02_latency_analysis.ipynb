{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Latency Analysis Dashboard\n",
                "\n",
                "This notebook analyzes the performance metrics from the FunctionGemma Agent logs:\n",
                "- Token generation latency\n",
                "- Tool execution time\n",
                "- Request patterns\n",
                "- Performance bottlenecks"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Install required packages\n",
                "# !pip install pandas matplotlib seaborn plotly ipywidgets\n",
                "\n",
                "import json\n",
                "import pandas as pd\n",
                "import numpy as np\n",
                "import matplotlib.pyplot as plt\n",
                "import seaborn as sns\n",
                "import plotly.express as px\n",
                "import plotly.graph_objects as go\n",
                "from plotly.subplots import make_subplots\n",
                "from datetime import datetime, timedelta\n",
                "import ipywidgets as widgets\n",
                "from IPython.display import display, HTML\n",
                "import warnings\n",
                "warnings.filterwarnings('ignore')\n",
                "\n",
                "# Configure plotting\n",
                "plt.style.use('seaborn-v0_8-darkgrid')\n",
                "sns.set_palette(\"husl\")\n",
                "\n",
                "# Configure pandas display\n",
                "pd.set_option('display.max_columns', None)\n",
                "pd.set_option('display.width', None)\n",
                "pd.set_option('display.max_colwidth', 50)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Load and Parse Log Data"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def parse_json_logs(log_file_path):\n",
                "    \"\"\"Parse JSON logs into a DataFrame.\"\"\"\n",
                "    logs = []\n",
                "    \n",
                "    try:\n",
                "        with open(log_file_path, 'r') as f:\n",
                "            for line in f:\n",
                "                line = line.strip()\n",
                "                if line:\n",
                "                    try:\n",
                "                        log_entry = json.loads(line)\n",
                "                        logs.append(log_entry)\n",
                "                    except json.JSONDecodeError:\n",
                "                        # Skip malformed lines\n",
                "                        continue\n",
                "    \n",
                "    df = pd.DataFrame(logs)\n",
                "    \n",
                "    # Convert timestamp\n",
                "    if 'timestamp' in df.columns:\n",
                "        df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
                "    \n",
                "    return df\n",
                "\n",
                "# Load the logs\n",
                "log_file = \"../logs/json_logs.log\"  # Update path as needed\n",
                "\n",
                "# For demo, create sample data if file doesn't exist\n",
                "import os\n",
                "if not os.path.exists(log_file):\n",
                "    print(\"Creating sample log data for demonstration...\")\n",
                "    create_sample_logs(log_file)\n",
                "\n",
                "# Parse logs\n",
                "df = parse_json_logs(log_file)\n",
                "\n",
                "print(f\"Loaded {len(df)} log entries\")\n",
                "print(f\"Time range: {df['timestamp'].min()} to {df['timestamp'].max()}\")\n",
                "\n",
                "# Display sample data\n",
                "print(\"\\nSample log entries:\")\n",
                "display(df.head())"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def create_sample_logs(filename):\n",
                "    \"\"\"Create sample log data for demonstration.\"\"\"\n",
                "    import random\n",
                "    \n",
                "    # Generate sample data\n",
                "    num_entries = 1000\n",
                "    base_time = datetime.now() - timedelta(hours=24)\n",
                "    \n",
                "    logs = []\n",
                "    for i in range(num_entries):\n",
                "        timestamp = base_time + timedelta(seconds=i * 86.4)  # Spread over 24 hours\n",
                "        \n",
                "        # Random latency values\n",
                "        token_latency = np.random.normal(100, 30)  # Mean 100ms, std 30ms\n",
                "        tool_latency = np.random.exponential(50)  # Mean 50ms\n",
                "        \n",
                "        log_entry = {\n",
                "            \"timestamp\": timestamp.isoformat(),\n",
                "            \"request_id\": f\"req-{i:06d}\",\n",
                "            \"token_latency_ms\": max(10, token_latency),\n",
                "            \"tool_execution_ms\": max(5, tool_latency),\n",
                "            \"total_latency_ms\": max(20, token_latency + tool_latency + np.random.normal(20, 5)),\n",
                "            \"tokens_used\": random.randint(50, 500),\n",
                "            \"tools_used\": random.randint(0, 3),\n",
                "            \"status\": random.choice([\"success\", \"success\", \"success\", \"error\"]),\n",
                "            \"model_version\": \"gemma-270m-v1.0\"\n",
                "        }\n",
                "        logs.append(log_entry)\n",
                "    \n",
                "    # Write to file\n",
                "    os.makedirs(os.path.dirname(filename), exist_ok=True)\n",
                "    with open(filename, 'w') as f:\n",
                "        for log in logs:\n",
                "            f.write(json.dumps(log) + '\\n')\n",
                "    \n",
                "    print(f\"Created {num_entries} sample log entries in {filename}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Overall Performance Overview"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Calculate key metrics\n",
                "metrics = {\n",
                "    'Total Requests': len(df),\n",
                "    'Success Rate': f\"{(df['status'] == 'success').mean() * 100:.1f}%\",\n",
                "    'Avg Total Latency': f\"{df['total_latency_ms'].mean():.2f}ms\",\n",
                "    'P95 Total Latency': f\"{df['total_latency_ms'].quantile(0.95):.2f}ms\",\n",
                "    'Avg Token Latency': f\"{df['token_latency_ms'].mean():.2f}ms\",\n",
                "    'Avg Tool Latency': f\"{df['tool_execution_ms'].mean():.2f}ms\",\n",
                "    'Avg Tokens per Request': f\"{df['tokens_used'].mean():.0f}\",\n",
                "    'Requests per Hour': f\"{len(df) / 24:.0f}\"\n",
                "}\n",
                "\n",
                "# Display metrics\n",
                "print(\"\\nðŸ“Š Performance Summary\")\n",
                "print(\"=\" * 50)\n",
                "for metric, value in metrics.items():\n",
                "    print(f\"{metric:.<30} {value}\")\n",
                "\n",
                "# Create a summary dashboard\n",
                "fig = make_subplots(\n",
                "    rows=2, cols=2,\n",
                "    subplot_titles=('Latency Distribution', 'Status Breakdown', 'Hourly Request Volume', 'Token Usage'),\n",
                "    specs=[[{\"secondary_y\": False}, {\"type\": \"domain\"}],\n",
                "            [{\"secondary_y\": False}, {\"secondary_y\": False}]]\n",
                ")\n",
                "\n",
                "# 1. Latency distribution histogram\n",
                "fig.add_trace(\n",
                "    go.Histogram(x=df['total_latency_ms'], name='Latency', nbinsx=50),\n",
                "    row=1, col=1\n",
                ")\n",
                "\n",
                "# 2. Status pie chart\n",
                "status_counts = df['status'].value_counts()\n",
                "fig.add_trace(\n",
                "    go.Pie(labels=status_counts.index, values=status_counts.values, name=\"Status\"),\n",
                "    row=1, col=2\n",
                ")\n",
                "\n",
                "# 3. Hourly request volume\n",
                "df['hour'] = df['timestamp'].dt.hour\n",
                "hourly_counts = df.groupby('hour').size().reset_index(name='count')\n",
                "fig.add_trace(\n",
                "    go.Bar(x=hourly_counts['hour'], y=hourly_counts['count'], name='Requests'),\n",
                "    row=2, col=1\n",
                ")\n",
                "\n",
                "# 4. Token usage scatter\n",
                "fig.add_trace(\n",
                "    go.Scatter(\n",
                "        x=df['tokens_used'],\n",
                "        y=df['token_latency_ms'],\n",
                "        mode='markers',\n",
                "        name='Tokens vs Latency',\n",
                "        opacity=0.6\n",
                "    ),\n",
                "    row=2, col=2\n",
                ")\n",
                "\n",
                "fig.update_layout(height=800, showlegend=False, title_text=\"Performance Overview Dashboard\")\n",
                "fig.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Token Generation Latency Analysis"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Detailed token latency analysis\n",
                "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
                "\n",
                "# 1. Token latency histogram\n",
                "axes[0, 0].hist(df['token_latency_ms'], bins=50, alpha=0.7, color='skyblue', edgecolor='black')\n",
                "axes[0, 0].axvline(df['token_latency_ms'].mean(), color='red', linestyle='--', label=f\"Mean: {df['token_latency_ms'].mean():.1f}ms\")\n",
                "axes[0, 0].axvline(df['token_latency_ms'].median(), color='green', linestyle='--', label=f\"Median: {df['token_latency_ms'].median():.1f}ms\")\n",
                "axes[0, 0].set_title('Token Generation Latency Distribution')\n",
                "axes[0, 0].set_xlabel('Latency (ms)')\n",
                "axes[0, 0].set_ylabel('Frequency')\n",
                "axes[0, 0].legend()\n",
                "\n",
                "# 2. Token latency over time\n",
                "df_sorted = df.sort_values('timestamp')\n",
                "axes[0, 1].plot(df_sorted['timestamp'], df_sorted['token_latency_ms'], alpha=0.5, linewidth=0.5)\n",
                "# Add rolling average\n",
                "rolling_avg = df_sorted['token_latency_ms'].rolling(window=50).mean()\n",
                "axes[0, 1].plot(df_sorted['timestamp'], rolling_avg, color='red', linewidth=2, label='50-point MA')\n",
                "axes[0, 1].set_title('Token Latency Over Time')\n",
                "axes[0, 1].set_xlabel('Time')\n",
                "axes[0, 1].set_ylabel('Latency (ms)')\n",
                "axes[0, 1].legend()\n",
                "axes[0, 1].tick_params(axis='x', rotation=45)\n",
                "\n",
                "# 3. Latency percentiles\n",
                "percentiles = [50, 75, 90, 95, 99]\n",
                "latency_percentiles = [df['token_latency_ms'].quantile(p/100) for p in percentiles]\n",
                "bars = axes[1, 0].bar([f'P{p}' for p in percentiles], latency_percentiles, color='lightcoral')\n",
                "axes[1, 0].set_title('Token Latency Percentiles')\n",
                "axes[1, 0].set_ylabel('Latency (ms)')\n",
                "# Add value labels\n",
                "for bar in bars:\n",
                "    height = bar.get_height()\n",
                "    axes[1, 0].text(bar.get_x() + bar.get_width()/2., height,\n",
                "                   f'{height:.1f}ms', ha='center', va='bottom')\n",
                "\n",
                "# 4. Latency vs Tokens Used\n",
                "axes[1, 1].scatter(df['tokens_used'], df['token_latency_ms'], alpha=0.3, s=20)\n",
                "# Add trend line\n",
                "z = np.polyfit(df['tokens_used'], df['token_latency_ms'], 1)\n",
                "p = np.poly1d(z)\n",
                "axes[1, 1].plot(df['tokens_used'], p(df['tokens_used']), \"r--\", alpha=0.8)\n",
                "axes[1, 1].set_title('Token Latency vs Tokens Generated')\n",
                "axes[1, 1].set_xlabel('Tokens Generated')\n",
                "axes[1, 1].set_ylabel('Latency (ms)')\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.show()\n",
                "\n",
                "# Print statistics\n",
                "print(\"\\nðŸ” Token Latency Statistics\")\n",
                "print(\"=\" * 50)\n",
                "print(f\"Mean: {df['token_latency_ms'].mean():.2f}ms\")\n",
                "print(f\"Median: {df['token_latency_ms'].median():.2f}ms\")\n",
                "print(f\"Std Dev: {df['token_latency_ms'].std():.2f}ms\")\n",
                "print(f\"Min: {df['token_latency_ms'].min():.2f}ms\")\n",
                "print(f\"Max: {df['token_latency_ms'].max():.2f}ms\")\n",
                "print(f\"P95: {df['token_latency_ms'].quantile(0.95):.2f}ms\")\n",
                "print(f\"P99: {df['token_latency_ms'].quantile(0.99):.2f}ms\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Tool Execution Time Analysis"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Filter requests that used tools\n",
                "df_with_tools = df[df['tools_used'] > 0].copy()\n",
                "\n",
                "print(f\"\\nðŸ› ï¸ Tool Execution Analysis\")\n",
                "print(\"=\" * 50)\n",
                "print(f\"Requests using tools: {len(df_with_tools)} ({len(df_with_tools)/len(df)*100:.1f}%)\")\n",
                "print(f\"Average tools per request: {df_with_tools['tools_used'].mean():.2f}\")\n",
                "\n",
                "# Create visualizations\n",
                "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
                "\n",
                "# 1. Tool execution time distribution\n",
                "axes[0, 0].hist(df['tool_execution_ms'], bins=50, alpha=0.7, color='lightgreen', edgecolor='black')\n",
                "axes[0, 0].axvline(df['tool_execution_ms'].mean(), color='red', linestyle='--', \n",
                "                label=f\"Mean: {df['tool_execution_ms'].mean():.1f}ms\")\n",
                "axes[0, 0].set_title('Tool Execution Time Distribution')\n",
                "axes[0, 0].set_xlabel('Time (ms)')\n",
                "axes[0, 0].set_ylabel('Frequency')\n",
                "axes[0, 0].legend()\n",
                "\n",
                "# 2. Tool count vs execution time\n",
                "tool_time_stats = df.groupby('tools_used')['tool_execution_ms'].agg(['mean', 'std', 'count']).reset_index()\n",
                "axes[0, 1].bar(tool_time_stats['tools_used'], tool_time_stats['mean'], \n",
                "               yerr=tool_time_stats['std'], capsize=5, color='orange', alpha=0.7)\n",
                "axes[0, 1].set_title('Tool Execution Time by Number of Tools')\n",
                "axes[0, 1].set_xlabel('Number of Tools Used')\n",
                "axes[0, 1].set_ylabel('Average Execution Time (ms)')\n",
                "\n",
                "# 3. Tool vs Token latency comparison\n",
                "axes[1, 0].scatter(df['token_latency_ms'], df['tool_execution_ms'], alpha=0.3, s=20)\n",
                "axes[1, 0].set_title('Token Latency vs Tool Execution Time')\n",
                "axes[1, 0].set_xlabel('Token Latency (ms)')\n",
                "axes[1, 0].set_ylabel('Tool Execution Time (ms)')\n",
                "\n",
                "# 4. Contribution to total latency\n",
                "df['token_contribution'] = df['token_latency_ms'] / df['total_latency_ms'] * 100\n",
                "df['tool_contribution'] = df['tool_execution_ms'] / df['total_latency_ms'] * 100\n",
                "df['other_contribution'] = 100 - df['token_contribution'] - df['tool_contribution']\n",
                "\n",
                "contributions = [\n",
                "    df['token_contribution'].mean(),\n",
                "    df['tool_contribution'].mean(),\n",
                "    df['other_contribution'].mean()\n",
                "]\n",
                "labels = ['Token Generation', 'Tool Execution', 'Other']\n",
                "colors = ['skyblue', 'lightgreen', 'lightcoral']\n",
                "\n",
                "axes[1, 1].pie(contributions, labels=labels, colors=colors, autopct='%1.1f%%', startangle=90)\n",
                "axes[1, 1].set_title('Average Latency Contribution Breakdown')\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.show()\n",
                "\n",
                "# Tool execution statistics\n",
                "print(\"\\nðŸ“Š Tool Execution Statistics\")\n",
                "print(\"=\" * 50)\n",
                "print(f\"Mean tool time: {df['tool_execution_ms'].mean():.2f}ms\")\n",
                "print(f\"Max tool time: {df['tool_execution_ms'].max():.2f}ms\")\n",
                "print(f\"P95 tool time: {df['tool_execution_ms'].quantile(0.95):.2f}ms\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5. Performance Patterns and Anomalies"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Detect anomalies in latency\n",
                "def detect_anomalies(data, threshold=2):\n",
                "    \"\"\"Detect anomalies using z-score threshold.\"\"\"\n",
                "    z_scores = np.abs((data - data.mean()) / data.std())\n",
                "    return z_scores > threshold\n",
                "\n",
                "# Find anomalies\n",
                "latency_anomalies = detect_anomalies(df['total_latency_ms'])\n",
                "anomaly_count = latency_anomalies.sum()\n",
                "\n",
                "print(f\"\\nðŸš¨ Anomaly Detection\")\n",
                "print(\"=\" * 50)\n",
                "print(f\"Anomalies detected: {anomaly_count} ({anomaly_count/len(df)*100:.1f}% of requests)\")\n",
                "\n",
                "if anomaly_count > 0:\n",
                "    print(\"\\nTop 5 anomalies:\")\n",
                "    anomaly_df = df[latency_anomalies].nlargest(5, 'total_latency_ms')\n",
                "    display(anomaly_df[['timestamp', 'total_latency_ms', 'token_latency_ms', \n",
                "                       'tool_execution_ms', 'tools_used', 'status']])\n",
                "\n",
                "# Visualize anomalies\n",
                "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
                "\n",
                "# 1. Latency over time with anomalies highlighted\n",
                "normal_mask = ~latency_anomalies\n",
                "axes[0].scatter(df[normal_mask]['timestamp'], df[normal_mask]['total_latency_ms'], \n",
                "               alpha=0.5, s=10, label='Normal', color='blue')\n",
                "axes[0].scatter(df[latency_anomalies]['timestamp'], df[latency_anomalies]['total_latency_ms'], \n",
                "               alpha=0.8, s=20, label='Anomaly', color='red')\n",
                "axes[0].set_title('Latency Over Time (Anomalies Highlighted)')\n",
                "axes[0].set_xlabel('Time')\n",
                "axes[0].set_ylabel('Total Latency (ms)')\n",
                "axes[0].legend()\n",
                "axes[0].tick_params(axis='x', rotation=45)\n",
                "\n",
                "# 2. Hourly performance pattern\n",
                "df['hour'] = df['timestamp'].dt.hour\n",
                "hourly_stats = df.groupby('hour')['total_latency_ms'].agg(['mean', 'std']).reset_index()\n",
                "\n",
                "axes[1].errorbar(hourly_stats['hour'], hourly_stats['mean'], yerr=hourly_stats['std'], \n",
                "                 marker='o', capsize=5, capthick=2, color='purple')\n",
                "axes[1].set_title('Average Latency by Hour of Day')\n",
                "axes[1].set_xlabel('Hour of Day')\n",
                "axes[1].set_ylabel('Average Latency (ms)')\n",
                "axes[1].set_xticks(range(0, 24, 2))\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 6. Interactive Performance Explorer"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Create interactive widgets for exploration\n",
                "style = {'description_width': 'initial'}\n",
                "\n",
                "# Time range selector\n",
                "start_date = widgets.DatePicker(\n",
                "    value=df['timestamp'].min().date(),\n",
                "    description='Start Date:',\n",
                "    style=style\n",
                ")\n",
                "\n",
                "end_date = widgets.DatePicker(\n",
                "    value=df['timestamp'].max().date(),\n",
                "    description='End Date:',\n",
                "    style=style\n",
                ")\n",
                "\n",
                "# Metric selector\n",
                "metric_dropdown = widgets.Dropdown(\n",
                "    options=[\n",
                "        ('Total Latency', 'total_latency_ms'),\n",
                "        ('Token Latency', 'token_latency_ms'),\n",
                "        ('Tool Execution Time', 'tool_execution_ms'),\n",
                "        ('Tokens Used', 'tokens_used')\n",
                "    ],\n",
                "    value='total_latency_ms',\n",
                "    description='Metric:',\n",
                "    style=style\n",
                ")\n",
                "\n",
                "# Aggregation selector\n",
                "agg_dropdown = widgets.Dropdown(\n",
                "    options=[('Mean', 'mean'), ('Median', 'median'), ('P95', lambda x: x.quantile(0.95)), ('Count', 'count')],\n",
                "    value='mean',\n",
                "    description='Aggregation:',\n",
                "    style=style\n",
                ")\n",
                "\n",
                "# Output widget\n",
                "output = widgets.Output()\n",
                "\n",
                "def update_analysis(_):\n",
                "    with output:\n",
                "        output.clear_output()\n",
                "        \n",
                "        # Filter data\n",
                "        mask = (df['timestamp'].dt.date >= start_date.value) & \\\n",
                "               (df['timestamp'].dt.date <= end_date.value)\n",
                "        filtered_df = df[mask]\n",
                "        \n",
                "        if len(filtered_df) == 0:\n",
                "            print(\"No data in selected range\")\n",
                "            return\n",
                "        \n",
                "        # Apply aggregation\n",
                "        if agg_dropdown.value == 'count':\n",
                "            filtered_df = filtered_df.groupby(filtered_df['timestamp'].dt.floor('H')).size().reset_index(name='value')\n",
                "            filtered_df.columns = ['timestamp', 'value']\n",
                "            ylabel = 'Request Count'\n",
                "        else:\n",
                "            if callable(agg_dropdown.value):\n",
                "                result = filtered_df.groupby(filtered_df['timestamp'].dt.floor('H'))[metric_dropdown.value].apply(agg_dropdown.value)\n",
                "            else:\n",
                "                result = filtered_df.groupby(filtered_df['timestamp'].dt.floor('H'))[metric_dropdown.value].agg(agg_dropdown.value)\n",
                "            filtered_df = result.reset_index(name='value')\n",
                "            ylabel = f'{metric_dropdown.value} ({agg_dropdown.value})'\n",
                "        \n",
                "        # Create plot\n",
                "        fig = go.Figure()\n",
                "        fig.add_trace(go.Scatter(\n",
                "            x=filtered_df['timestamp'],\n",
                "            y=filtered_df['value'],\n",
                "            mode='lines+markers',\n",
                "            name=metric_dropdown.value\n",
                "        ))\n",
                "        \n",
                "        fig.update_layout(\n",
                "            title=f'{metric_dropdown.value} Over Time',\n",
                "            xaxis_title='Time',\n",
                "            yaxis_title=ylabel,\n",
                "            hovermode='x unified'\n",
                "        )\n",
                "        \n",
                "        fig.show()\n",
                "        \n",
                "        # Print summary\n",
                "        print(f\"\\nðŸ“ˆ Analysis Summary\")\n",
                "        print(f\"Period: {start_date.value} to {end_date.value}\")\n",
                "        print(f\"Total requests: {len(mask[mask])}\")\n",
                "        print(f\"Average {metric_dropdown.value}: {df[mask][metric_dropdown.value].mean():.2f}\")\n",
                "\n",
                "# Create dashboard\n",
                "dashboard = widgets.VBox([\n",
                "    widgets.HBox([start_date, end_date]),\n",
                "    widgets.HBox([metric_dropdown, agg_dropdown]),\n",
                "    output\n",
                "])\n",
                "\n",
                "# Connect widgets\n",
                "start_date.observe(update_analysis, names='value')\n",
                "end_date.observe(update_analysis, names='value')\n",
                "metric_dropdown.observe(update_analysis, names='value')\n",
                "agg_dropdown.observe(update_analysis, names='value')\n",
                "\n",
                "# Display dashboard\n",
                "print(\"\\nðŸ“Š Interactive Performance Dashboard\")\n",
                "print(\"=\" * 50)\n",
                "print(\"Use the controls above to explore performance metrics\")\n",
                "display(dashboard)\n",
                "\n",
                "# Initial plot\n",
                "update_analysis(None)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 7. Performance Optimization Recommendations"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def generate_recommendations(df):\n",
                "    \"\"\"Generate performance optimization recommendations.\"\"\"\n",
                "    recommendations = []\n",
                "    \n",
                "    # Check overall latency\n",
                "    p95_latency = df['total_latency_ms'].quantile(0.95)\n",
                "    if p95_latency > 500:\n",
                "        recommendations.append({\n",
                "            'category': 'High Latency',\n",
                "            'priority': 'High',\n",
                "            'issue': f'P95 latency is {p95_latency:.0f}ms (>500ms threshold)',\n",
                "            'recommendation': 'Consider enabling request batching or upgrading hardware'\n",
                "        })\n",
                "    \n",
                "    # Check token generation efficiency\n",
                "    tokens_per_ms = df['tokens_used'] / df['token_latency_ms']\n",
                "    if tokens_per_ms.mean() < 1:\n",
                "        recommendations.append({\n",
                "            'category': 'Token Generation',\n",
                "            'priority': 'Medium',\n",
                "            'issue': f'Low token generation rate: {tokens_per_ms.mean():.2f} tokens/ms',\n",
                "            'recommendation': 'Optimize model quantization or use GPU acceleration'\n",
                "        })\n",
                "    \n",
                "    # Check tool execution\n",
                "    avg_tool_time = df[df['tools_used'] > 0]['tool_execution_ms'].mean()\n",
                "    if avg_tool_time > 100:\n",
                "        recommendations.append({\n",
                "            'category': 'Tool Performance',\n",
                "            'priority': 'Medium',\n",
                "            'issue': f'High tool execution time: {avg_tool_time:.0f}ms average',\n",
                "            'recommendation': 'Optimize tool code or add caching for frequent operations'\n",
                "        })\n",
                "    \n",
                "    # Check error rate\n",
                "    error_rate = (df['status'] == 'error').mean() * 100\n",
                "    if error_rate > 5:\n",
                "        recommendations.append({\n",
                "            'category': 'Reliability',\n",
                "            'priority': 'High',\n",
                "            'issue': f'High error rate: {error_rate:.1f}%',\n",
                "            'recommendation': 'Investigate error patterns and implement better error handling'\n",
                "        })\n",
                "    \n",
                "    # Check for patterns\n",
                "    hourly_latency = df.groupby(df['timestamp'].dt.hour)['total_latency_ms'].mean()\n",
                "    peak_hour = hourly_latency.idxmax()\n",
                "    peak_latency = hourly_latency.max()\n",
                "    avg_latency = hourly_latency.mean()\n",
                "    \n",
                "    if peak_latency > avg_latency * 1.5:\n",
                "        recommendations.append({\n",
                "            'category': 'Capacity Planning',\n",
                "            'priority': 'Low',\n",
                "            'issue': f'Peak latency at hour {peak_hour} is {peak_latency/avg_latency:.1f}x average',\n",
                "            'recommendation': 'Consider auto-scaling or load balancing for peak hours'\n",
                "        })\n",
                "    \n",
                "    return recommendations\n",
                "\n",
                "# Generate and display recommendations\n",
                "recommendations = generate_recommendations(df)\n",
                "\n",
                "print(\"\\nðŸ’¡ Performance Optimization Recommendations\")\n",
                "print(\"=\" * 60)\n",
                "\n",
                "if recommendations:\n",
                "    # Create DataFrame for better display\n",
                "    rec_df = pd.DataFrame(recommendations)\n",
                "    display(rec_df[['category', 'priority', 'issue', 'recommendation']])\n",
                "    \n",
                "    # Priority breakdown\n",
                "    print(\"\\nðŸ“‹ Priority Breakdown:\")\n",
                "    priority_counts = rec_df['priority'].value_counts()\n",
                "    for priority, count in priority_counts.items():\n",
                "        print(f\"  {priority}: {count} recommendation(s)\")\n",
                "else:\n",
                "    print(\"âœ… No major performance issues detected. System is performing well!\")\n",
                "\n",
                "# Create a summary visualization\n",
                "if recommendations:\n",
                "    fig, ax = plt.subplots(figsize=(10, 6))\n",
                "    \n",
                "    # Group by category\n",
                "    category_counts = rec_df['category'].value_counts()\n",
                "    \n",
                "    # Create horizontal bar chart\n",
                "    bars = ax.barh(category_counts.index, category_counts.values, color='steelblue')\n",
                "    \n",
                "    # Customize\n",
                "    ax.set_title('Performance Issues by Category', fontweight='bold', fontsize=14)\n",
                "    ax.set_xlabel('Number of Issues')\n",
                "    \n",
                "    # Add value labels\n",
                "    for bar in bars:\n",
                "        width = bar.get_width()\n",
                "        ax.text(width + 0.1, bar.get_y() + bar.get_height()/2,\n",
                "               f'{int(width)}', ha='left', va='center')\n",
                "    \n",
                "    plt.tight_layout()\n",
                "    plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 8. Export Report"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def generate_performance_report(df, recommendations):\n",
                "    \"\"\"Generate a comprehensive performance report.\"\"\"\n",
                "    \n",
                "    report = f\"\"\"\n",
                "# FunctionGemma Agent Performance Report\n",
                "Generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n",
                "\n",
                "## Executive Summary\n",
                "- Total Requests Analyzed: {len(df):,}\n",
                "- Analysis Period: {df['timestamp'].min().strftime('%Y-%m-%d')} to {df['timestamp'].max().strftime('%Y-%m-%d')}\n",
                "- Overall Success Rate: {(df['status'] == 'success').mean() * 100:.1f}%\n",
                "- Average Response Time: {df['total_latency_ms'].mean():.2f}ms\n",
                "- P95 Response Time: {df['total_latency_ms'].quantile(0.95):.2f}ms\n",
                "\n",
                "## Performance Metrics\n",
                "\n",
                "### Latency Analysis\n",
                "- Token Generation: {df['token_latency_ms'].mean():.2f}ms average\n",
                "- Tool Execution: {df['tool_execution_ms'].mean():.2f}ms average\n",
                "- Other Overhead: {df['total_latency_ms'].mean() - df['token_latency_ms'].mean() - df['tool_execution_ms'].mean():.2f}ms\n",
                "\n",
                "### Throughput Metrics\n",
                "- Requests per Hour: {len(df) / ((df['timestamp'].max() - df['timestamp'].min()).total_seconds() / 3600):.1f}\n",
                "- Average Tokens per Request: {df['tokens_used'].mean():.0f}\n",
                "- Tool Usage Rate: {(df['tools_used'] > 0).mean() * 100:.1f}%\n",
                "\n",
                "## Issues Identified\n",
                "\"\"\"\n",
                "    \n",
                "    if recommendations:\n",
                "        for rec in recommendations:\n",
                "            report += f\"\\n### {rec['category']} (Priority: {rec['priority']})\\n\"\n",
                "            report += f\"- Issue: {rec['issue']}\\n\"\n",
                "            report += f\"- Recommendation: {rec['recommendation']}\\n\\n\"\n",
                "    else:\n",
                "        report += \"\\nâœ… No critical performance issues detected.\\n\\n\"\n",
                "    \n",
                "    report += f\"\"\"\n",
                "## Recommendations Summary\n",
                "- High Priority: {len([r for r in recommendations if r['priority'] == 'High'])} items\n",
                "- Medium Priority: {len([r for r in recommendations if r['priority'] == 'Medium'])} items\n",
                "- Low Priority: {len([r for r in recommendations if r['priority'] == 'Low'])} items\n",
                "\n",
                "## Next Steps\n",
                "1. Address high-priority issues immediately\n",
                "2. Schedule medium-priority optimizations for next sprint\n",
                "3. Monitor low-priority trends and plan future improvements\n",
                "4. Set up automated alerts for key metrics\n",
                "\n",
                "---\n",
                "*Report generated by FunctionGemma Agent Performance Analysis Notebook*\n",
                "\"\"\"\n",
                "    \n",
                "    return report\n",
                "\n",
                "# Generate and save report\n",
                "report = generate_performance_report(df, recommendations)\n",
                "\n",
                "# Save to file\n",
                "report_filename = f\"performance_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.md\"\n",
                "with open(report_filename, 'w') as f:\n",
                "    f.write(report)\n",
                "\n",
                "print(f\"\\nðŸ“„ Report saved to: {report_filename}\")\n",
                "print(\"\\nðŸ“Š Performance analysis completed successfully!\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.10.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}